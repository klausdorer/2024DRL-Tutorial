{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d7acd9-e134-4599-b0ad-95c2f652f2a5",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning with Stable Baselines 3\n",
    "\n",
    "In this exercise, you'll train a walking behavior using stable baselines, a library made for deep reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f30340-4197-42a6-b488-4ca44e316fcd",
   "metadata": {},
   "source": [
    "First, let's import the necessary parts of the libraries for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a690a-8d49-4fa4-a9af-0caf43f00151",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0845db-74f8-4a3a-b6fd-da5b275c91c9",
   "metadata": {},
   "source": [
    "**Important:** should this step yield any errors, check if this notebook runs in the correct conda environment. Without the imports working, none of the following steps will work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df99342-99ea-4afa-889e-b896f9b41c39",
   "metadata": {},
   "source": [
    "Next, we need to create the environments for which we want to learn a walking behavior. The goal of this environment is to make the robot move to the right side (positive x direction). For more information about the HalfCheetah environment, visit https://gymnasium.farama.org/environments/mujoco/half_cheetah/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d300aa-5f29-4f04-b014-f5ba528ca8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_id = \"HalfCheetah-v4\"\n",
    "env_raw = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "# A seperate environment is required for evaluation\n",
    "eval_env_raw = gym.make(env_id, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21dde0d-5197-47ec-9c93-59b38ee45cf2",
   "metadata": {},
   "source": [
    "With Deep Reinforcement Learning, an agent tries to maximize the reward it gets for the actions he performs. Your task is now to write a function to calculate the reward for the agent.\n",
    "\n",
    "The following are the most important values that you can use for calculating the reward.\n",
    "- `action` is an array containing the actions for the agent. The documentation linked above explains what these values represent.\n",
    "- Use `self.env.data.xpos` to get the positions of the robot's body parts. As calling the \"inner\" environment's `step` method advances the simulation using the generated action, it makes a difference if you obtain those positions before calling `step` or after. This is a two-dimensional environment. Therefore, `self.env.data.xpos[?][1]` (the y-coordinate) is of no interest for all body parts. The following information is taken from the definition of the environment: https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/mujoco/assets/half_cheetah.xml. The body parts should usually appear in the same order in the array as defined in the XML file using `<body>` tags.\n",
    "  * `self.env.data.xpos[1]`: torso\n",
    "  * `self.env.data.xpos[2]`: back thigh\n",
    "  * `self.env.data.xpos[3]`: back shin\n",
    "  * `self.env.data.xpos[4]`: back foot\n",
    "  * `self.env.data.xpos[5]`: front thigh\n",
    "  * `self.env.data.xpos[6]`: front shin\n",
    "  * `self.env.data.xpos[7]`: front foot\n",
    "- `self.dt` contains the time elapsed during the simulation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1cf89-4f57-4ff4-8b25-19cd899e70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalfCheetahRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def step(self, action):\n",
    "        x_position_torso_before = self.env.data.xpos[1][0]\n",
    "        \n",
    "        # Run the simulation and apply the action\n",
    "        observation, _, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        x_position_torso_after = self.env.data.xpos[1][0]\n",
    "        \n",
    "        # TODO: calculate the reward here\n",
    "        reward = 0\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "# Wrap the existing environments\n",
    "env = HalfCheetahRewardWrapper(env_raw)\n",
    "eval_env = Monitor(HalfCheetahRewardWrapper(eval_env_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc0e7b0-8a04-44dc-ad3c-049ee012bfc7",
   "metadata": {},
   "source": [
    "Now, we need to create the model. It is initialized with a random policy.\n",
    "For this exercise, we'll use the PPO (Proximal Policy Optimization) algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4bf73a-52c9-4d85-8219-632811776a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c861c79-7203-40b6-b0cb-38e73f2e5e4c",
   "metadata": {},
   "source": [
    "Let's see how the random policy performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d382dac1-c1bc-4021-9ed8-803915297a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward, std_deviation = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
    "print(\"mean reward:\", mean_reward)\n",
    "print(\"standard deviation:\", std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6dce8f-d694-41b2-a275-ab6cb001dd54",
   "metadata": {},
   "source": [
    "Now, it's time for learning how to walk. You can adjust the number of timesteps to learn on based on the strength of your computer. More timesteps take longer but may improve the resulting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beae75e2-5743-44d7-9d9b-9ea48978c362",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0575cf68-565d-4af5-9fe8-84e815e6ff36",
   "metadata": {},
   "source": [
    "Another evaluation is required to measure if an improvement was made to the model compared to its initial random state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347df854-1b69-425c-8c73-4b42d008817e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward, std_deviation = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
    "print(\"mean reward:\", mean_reward)\n",
    "print(\"standard deviation:\", std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ecbe2-46a5-40d1-b2b4-39da72550348",
   "metadata": {},
   "source": [
    "You can export a GIF of your trained behavior using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e21711-60b1-40ef-8c78-ec293987fcb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "filename = \"halfcheetah.gif\"\n",
    "\n",
    "images = []\n",
    "obs = model.env.reset()\n",
    "img = model.env.render()\n",
    "start_pos = model.env.envs[0].data.xpos[1][0]\n",
    "for _ in range(200):\n",
    "    images.append(img)\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _, _ = model.env.step(action)\n",
    "    img = model.env.render()\n",
    "\n",
    "end_pos = model.env.envs[0].data.xpos[1][0]\n",
    "print(f\"Distance travelled: {end_pos - start_pos}m\")\n",
    "\n",
    "imageio.mimsave(filename, [np.array(img) for img in images], duration=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecb1b3d-fb55-4f30-be2c-41347e017b13",
   "metadata": {},
   "source": [
    "Within the visualization, one side of a (black or white) square is $\\frac{2}{3}$ meters long."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d3b41f-d0ab-4bad-8a36-ee3175bd3c3c",
   "metadata": {},
   "source": [
    "If you are interested in training robots in other environments, check out the already existing environments in Gymnasium: https://gymnasium.farama.org/environments/mujoco/.\n",
    "You should be able to use another environment id and everything else in the notebook should mostly continue to work. Of course, you then need to find out yourself what the positions in `self.env.data.xpos` represent for your chosen environment.\n",
    "Note: as of writing this (August 2023), there seem to be some problems with the Hopper and the Walker2D environment that prevent these environments from being used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d885a1e",
   "metadata": {},
   "source": [
    "Now let's see how high the robot is able to jump. Once again, we need to define a reward function.\n",
    "It might make sense to manually set `terminated` and `truncated` in this case.\n",
    "Have a look at https://gymnasium.farama.org/api/env/#gymnasium.Env.step for an explanation of their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalfCheetahJumpRewardWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Run the simulation and apply the action\n",
    "        observation, _, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        # TODO: calculate the reward here\n",
    "        reward = 0\n",
    "        \n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "# Wrap the existing environments\n",
    "jump_env = HalfCheetahJumpRewardWrapper(env_raw)\n",
    "jump_eval_env = Monitor(HalfCheetahJumpRewardWrapper(eval_env_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3dc40",
   "metadata": {},
   "source": [
    "Again, we need to create a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2937a633",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jump_model = PPO(\"MlpPolicy\", jump_env, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b78c7",
   "metadata": {},
   "source": [
    "Let's have a view at performance of the random policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089a18c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward, std_deviation = evaluate_policy(jump_model, jump_eval_env, n_eval_episodes=100)\n",
    "print(\"mean reward:\", mean_reward)\n",
    "print(\"standard deviation:\", std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045c7df",
   "metadata": {},
   "source": [
    "Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c63314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jump_model.learn(total_timesteps=200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5defac0e",
   "metadata": {},
   "source": [
    "Evaluate the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d239ff47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_reward, std_deviation = evaluate_policy(jump_model, jump_eval_env, n_eval_episodes=100)\n",
    "print(\"mean reward:\", mean_reward)\n",
    "print(\"standard deviation:\", std_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6899bf",
   "metadata": {},
   "source": [
    "Of course, you can export another GIF of your trained jumping behavior using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16cc34b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "filename = \"halfcheetah_jump.gif\"\n",
    "\n",
    "images = []\n",
    "obs = jump_model.env.reset()\n",
    "img = jump_model.env.render()\n",
    "for _ in range(100):\n",
    "    images.append(img)\n",
    "    action, _ = jump_model.predict(obs)\n",
    "    obs, _, _, _ = jump_model.env.step(action)\n",
    "    img = jump_model.env.render()\n",
    "\n",
    "imageio.mimsave(filename, [np.array(img) for img in images], duration=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drl3]",
   "language": "python",
   "name": "conda-env-drl3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
